{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from my_utils import id2section, section2id\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINING_DIR = '1-mining'\n",
    "PREPROCESSING_DIR = '2-preprocessing'\n",
    "DATA_DIR = 'lol'\n",
    "\n",
    "mining_dir = Path(MINING_DIR) / DATA_DIR\n",
    "preprocessing_dir = Path(PREPROCESSING_DIR) / DATA_DIR\n",
    "preprocessing_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mining_dir / 'info.json') as f:\n",
    "    news_infos_by_date = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['date', 'ranking', 'thumbnail', 'headline', 'lede', 'office', 'view']\n",
    "rows = []\n",
    "for date, news_infos in news_infos_by_date.items():\n",
    "    for rank, info in enumerate(news_infos, 1):\n",
    "        thumbnail = int(info['thumbnail'] is not None)\n",
    "        rows.append([date, rank, thumbnail, info['title'], info['subContent'], info['officeName'], info['totalCount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ranking</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>headline</th>\n",
       "      <th>lede</th>\n",
       "      <th>office</th>\n",
       "      <th>view</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20150801</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[롤챔스 핫매치 리뷰] '불량 학생' 노동현의 수능 대박 스토리</td>\n",
       "      <td>\"탈선했어요!\"\"수능 만점!\" 2015년 7월 31일, KOO 타이거즈와 kt 롤스...</td>\n",
       "      <td>인벤</td>\n",
       "      <td>198042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20150801</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[롤챔스 섬머] 스베누 소닉붐 뉴클리어 - 사신 인터뷰, 앞으로는 승리만 안겨드리겠다</td>\n",
       "      <td>금일(1일) 롤챔스 섬머 시즌 사상 스베누 소닉붐이 첫 번째 승리를 기록했다. 15...</td>\n",
       "      <td>헝그리앱</td>\n",
       "      <td>8657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20150801</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[롤챔스 섬머] 뱅 배준식 인터뷰. 솔로랭크를 올리는데는 멘탈이 중요!</td>\n",
       "      <td>Q. 오늘 승리한 소감은 어떻게 되나?오늘 경기에 오랜만에 배성웅(벵기)형와 이지훈...</td>\n",
       "      <td>헝그리앱</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20150801</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[롬챔스 섬머] 스베누 소닉붐, 꼴찌의 반격 개시! 그 동안 설움을 설욕한 대망의 ...</td>\n",
       "      <td>롤챔스 섬머 2경기 2세트는 기세를 올린 스베누 소닉붐이 승리를 가져갔다. 이 때문...</td>\n",
       "      <td>헝그리앱</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150801</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[롬챔스 섬머] 스베누 소닉붐, 한타 대승 기반으로 2세트 승리</td>\n",
       "      <td>롤챔스 섬머 2경기 2세트가 시작됐다. 1세트때 진에어 그린윙스에게 억눌린 스베누 ...</td>\n",
       "      <td>헝그리앱</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51811</th>\n",
       "      <td>20200430</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>[LCK 승강전] 다이브 받아치면 곧 승리! 샌드박스, 서라벌에 1세트 선승</td>\n",
       "      <td>30일 종각 LoL 파크에서 열린 열린 2020 LCK 섬머 스플릿 승강전 최종전 ...</td>\n",
       "      <td>인벤</td>\n",
       "      <td>1166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51812</th>\n",
       "      <td>20200430</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>[롤챔스 승강전] 서라벌-샌드박스, 베스트 라인업으로 맞대결</td>\n",
       "      <td>LCK 2020 서머 최종전에서 대결하는 서라벌 게이밍(위)과 샌드박스 게이밍(사진...</td>\n",
       "      <td>데일리e스포츠</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51813</th>\n",
       "      <td>20200430</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>‘일방적인 전투 승리’…샌드박스게이밍, 서라벌게이밍 상대로 1세트 승리</td>\n",
       "      <td>30일 ‘2020 우리은행 LCK Spring Split’ 승강전 최종진출전에서는 ...</td>\n",
       "      <td>엑스포츠뉴스</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51814</th>\n",
       "      <td>20200430</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>[LCK 승강전] 샌드박스, 절묘한 카운터로 손쉽게 기선 제압</td>\n",
       "      <td>[OSEN=고용준 기자] LCK 최후의 승강전에서 샌드박스가 먼저 웃었다. 다이브 ...</td>\n",
       "      <td>OSEN</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51815</th>\n",
       "      <td>20200430</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>[LCK 승강전] 샌드박스, 집중력 앞세워 혼전 승리 2-0 리드</td>\n",
       "      <td>[OSEN=고용준 기자] 잡은 기회는 확실하게 놓치지 않았다. 샌드박스가 LCK 최...</td>\n",
       "      <td>OSEN</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51816 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  ranking  thumbnail  \\\n",
       "0      20150801        1          1   \n",
       "1      20150801        2          1   \n",
       "2      20150801        3          1   \n",
       "3      20150801        4          1   \n",
       "4      20150801        5          1   \n",
       "...         ...      ...        ...   \n",
       "51811  20200430       26          1   \n",
       "51812  20200430       27          1   \n",
       "51813  20200430       28          1   \n",
       "51814  20200430       29          1   \n",
       "51815  20200430       30          1   \n",
       "\n",
       "                                                headline  \\\n",
       "0                    [롤챔스 핫매치 리뷰] '불량 학생' 노동현의 수능 대박 스토리   \n",
       "1        [롤챔스 섬머] 스베누 소닉붐 뉴클리어 - 사신 인터뷰, 앞으로는 승리만 안겨드리겠다   \n",
       "2                [롤챔스 섬머] 뱅 배준식 인터뷰. 솔로랭크를 올리는데는 멘탈이 중요!   \n",
       "3      [롬챔스 섬머] 스베누 소닉붐, 꼴찌의 반격 개시! 그 동안 설움을 설욕한 대망의 ...   \n",
       "4                    [롬챔스 섬머] 스베누 소닉붐, 한타 대승 기반으로 2세트 승리   \n",
       "...                                                  ...   \n",
       "51811         [LCK 승강전] 다이브 받아치면 곧 승리! 샌드박스, 서라벌에 1세트 선승   \n",
       "51812                  [롤챔스 승강전] 서라벌-샌드박스, 베스트 라인업으로 맞대결   \n",
       "51813            ‘일방적인 전투 승리’…샌드박스게이밍, 서라벌게이밍 상대로 1세트 승리   \n",
       "51814                 [LCK 승강전] 샌드박스, 절묘한 카운터로 손쉽게 기선 제압   \n",
       "51815               [LCK 승강전] 샌드박스, 집중력 앞세워 혼전 승리 2-0 리드   \n",
       "\n",
       "                                                    lede   office    view  \n",
       "0      \"탈선했어요!\"\"수능 만점!\" 2015년 7월 31일, KOO 타이거즈와 kt 롤스...       인벤  198042  \n",
       "1      금일(1일) 롤챔스 섬머 시즌 사상 스베누 소닉붐이 첫 번째 승리를 기록했다. 15...     헝그리앱    8657  \n",
       "2      Q. 오늘 승리한 소감은 어떻게 되나?오늘 경기에 오랜만에 배성웅(벵기)형와 이지훈...     헝그리앱     416  \n",
       "3      롤챔스 섬머 2경기 2세트는 기세를 올린 스베누 소닉붐이 승리를 가져갔다. 이 때문...     헝그리앱       0  \n",
       "4      롤챔스 섬머 2경기 2세트가 시작됐다. 1세트때 진에어 그린윙스에게 억눌린 스베누 ...     헝그리앱       0  \n",
       "...                                                  ...      ...     ...  \n",
       "51811  30일 종각 LoL 파크에서 열린 열린 2020 LCK 섬머 스플릿 승강전 최종전 ...       인벤    1166  \n",
       "51812  LCK 2020 서머 최종전에서 대결하는 서라벌 게이밍(위)과 샌드박스 게이밍(사진...  데일리e스포츠    1057  \n",
       "51813  30일 ‘2020 우리은행 LCK Spring Split’ 승강전 최종진출전에서는 ...   엑스포츠뉴스     835  \n",
       "51814  [OSEN=고용준 기자] LCK 최후의 승강전에서 샌드박스가 먼저 웃었다. 다이브 ...     OSEN     490  \n",
       "51815  [OSEN=고용준 기자] 잡은 기회는 확실하게 놓치지 않았다. 샌드박스가 LCK 최...     OSEN     460  \n",
       "\n",
       "[51816 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer_df = pd.DataFrame(rows, columns=columns)\n",
    "outer_df.to_csv(preprocessing_dir / 'news-outer.csv', index=False)\n",
    "outer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME_PATTERN = re.compile(r'(\\d{4})\\.(\\d{2})\\.(\\d{2})\\. (?:오전|오후) (\\d{1,2}):(\\d{2})')\n",
    "\n",
    "def parse_datetime(x: str) -> str:\n",
    "    x = x.replace('기사입력', '').replace('최종수정', '').strip()\n",
    "    Y, m, d, H, M = [int(x) for x in DATETIME_PATTERN.match(x).groups()]\n",
    "    pm = '오후' in x\n",
    "    if pm and H < 12:\n",
    "        H += 12\n",
    "    return datetime(year=Y, month=m, day=d, hour=H, minute=M).strftime('%Y%m%d %H:%M')\n",
    "\n",
    "REPORTER_PATTERN_STRING = r'[가-힣]{2,4} ?기자'\n",
    "EMAIL_PATTERN_STRING = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
    "\n",
    "METADATA_PATTERN_1 = re.compile(r'[\\[(<][^>)\\]]*?%s[^>)\\]]*?[>)\\]](\\s*\\.)?' % REPORTER_PATTERN_STRING)\n",
    "METADATA_PATTERN_2 = re.compile(r'[\\[(<][^>)\\]]*?%s[^>)\\]]*?[>)\\]](\\s*\\.)?' % EMAIL_PATTERN_STRING)\n",
    "METADATA_PATTERN_3 = re.compile(r'(?<=\\.) (?:[^.]*?L.A)?[^.]*?%s.*$' % REPORTER_PATTERN_STRING)\n",
    "METADATA_PATTERN_4 = re.compile(r'(?<=\\.)/? [^.]*?%s.*$' % EMAIL_PATTERN_STRING)\n",
    "METADATA_PATTERN_5 = re.compile(r'(?<=\\.) +Copyrights? ⓒ.*$')\n",
    "METADATA_PATTERN_6 = re.compile(r'(?<=\\.) +<생활경제팀>.*$')\n",
    "METADATA_PATTERN_7 = re.compile(r'(?<=\\.) +스포츠경향 뉴스를.*$')\n",
    "\n",
    "NODOT_LINE_PATTERN = re.compile(r'(?<!\\.|\\n)\\n')\n",
    "SPACES_PATTERN = re.compile(r'\\s+')\n",
    "\n",
    "EMAIL_PATTERN = re.compile(EMAIL_PATTERN_STRING)\n",
    "URL_PATTERN = re.compile(r'(?:\\(\\s)(?:https?|ftp)://[^\\s/$.?#].[^\\s)]*?\\)?')\n",
    "\n",
    "\n",
    "def extract_text_from_html(soup: BeautifulSoup, office: str='', remove_metadata: bool=False) -> str:\n",
    "    # 태그 제거\n",
    "    blacklist = ['script', 'noscript', 'style', 'em', 'table']\n",
    "    for tagname in blacklist:\n",
    "        for x in soup.select(tagname):\n",
    "            x.decompose()\n",
    "    \n",
    "    # 라인별 전후방 공백 제거\n",
    "    x = soup.text.strip()\n",
    "    x = '\\n'.join(a.strip() for a in x.splitlines())\n",
    "\n",
    "    # 공백 축소\n",
    "    x = NODOT_LINE_PATTERN.sub('.\\n', x)\n",
    "    x = SPACES_PATTERN.sub(' ', x)\n",
    "    \n",
    "    # 메타데이터 제거\n",
    "    if remove_metadata:\n",
    "        x_old = x\n",
    "        x = METADATA_PATTERN_1.sub('', x)\n",
    "        x = METADATA_PATTERN_2.sub('', x)\n",
    "        if office:\n",
    "            x = re.sub(r'[\\[(<][^>)\\]]*?%s[^>)\\]]*?[>)\\]](\\s*\\.)?' % office, '', x)\n",
    "\n",
    "        x_tmp = x\n",
    "        x = METADATA_PATTERN_3.sub('', x)\n",
    "        x = METADATA_PATTERN_4.sub('', x)\n",
    "        x = METADATA_PATTERN_5.sub('', x)\n",
    "        x = METADATA_PATTERN_6.sub('', x)\n",
    "        x = METADATA_PATTERN_7.sub('', x)\n",
    "        if x == x_tmp:\n",
    "            print(office)\n",
    "            print(x_tmp)\n",
    "            x += '[UNCHANGED!!!]'\n",
    "    \n",
    "    # URL, EMAIL 제거\n",
    "    x = EMAIL_PATTERN.sub(' ', x)\n",
    "    x = URL_PATTERN.sub(' ', x)\n",
    "    return x.strip()\n",
    "\n",
    "\n",
    "def parse_row_from_news(soup: BeautifulSoup, **kwargs) -> list:\n",
    "    soup = soup.select_one('#main_content')\n",
    "    title = soup.select_one('#articleTitle').text.strip()\n",
    "    pick = int(soup.select_one('.article_header > .head_channel').attrs['style'] == 'display: block;')\n",
    "    paper = int(soup.select_one('.article_info > .sponsor > .sponsor_newspaper') is not None)\n",
    "    \n",
    "    dates = [parse_datetime(x.text) for x in soup.select('.article_info > .sponsor > .t11')]\n",
    "    date_input = dates[0]\n",
    "    date_modify = dates[1] if len(dates) == 2 else dates[0]\n",
    "\n",
    "    body = soup.select_one('#articleBodyContents')\n",
    "    img = int(body.select_one('img') is not None)\n",
    "    vod = int(body.select_one('.vod_area') is not None)\n",
    "    contents = extract_text_from_html(body, **kwargs)\n",
    "    return [title, pick, paper, date_input, date_modify, img, vod, contents]\n",
    "\n",
    "\n",
    "def parse_row_from_entertain(soup: BeautifulSoup, **kwargs) -> list:\n",
    "    soup = soup.select_one('#content')\n",
    "    title = soup.select_one('.end_tit').text.strip()\n",
    "    pick = 0\n",
    "    paper = 0\n",
    "\n",
    "    dates = [parse_datetime(x.text) for x in soup.select('.article_info > .author > em')]\n",
    "    date_input = dates[0]\n",
    "    date_modify = dates[1] if len(dates) == 2 else dates[0]\n",
    "\n",
    "    body = soup.select_one('#articeBody')\n",
    "    img = int(body.select_one('img') is not None)\n",
    "    vod = int(body.select_one('.vod_area') is not None)\n",
    "    contents = extract_text_from_html(body, **kwargs)\n",
    "    return [title, pick, paper, date_input, date_modify, img, vod, contents]\n",
    "\n",
    "\n",
    "def parse_row_from_sports(soup: BeautifulSoup, **kwargs) -> list:\n",
    "    soup = soup.select_one('#content')\n",
    "    if soup.select_one('.column_wrap'):\n",
    "        title = soup.select_one('.default_h > .info_tit').text.strip()\n",
    "        date_input = parse_datetime(soup.select_one('.default_h > .info_date').text)\n",
    "        date_modify = date_input\n",
    "    else:\n",
    "        title = soup.select_one('.news_headline > .title').text.strip()\n",
    "        dates = [parse_datetime(x.text) for x in soup.select('.news_headline > .info > span')]\n",
    "        date_input = dates[0]\n",
    "        date_modify = dates[1] if len(dates) == 2 else dates[0]\n",
    "\n",
    "    body = soup.select_one('#newsEndContents')\n",
    "    img = int(body.select_one('img') is not None)\n",
    "    vod = int(body.select_one('.vod_area') is not None)\n",
    "    contents = extract_text_from_html(body, **kwargs)\n",
    "    return [title, date_input, date_modify, img, vod, contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['title', 'date_input', 'date_modify', 'img', 'vod', 'contents']\n",
    "new_rows = []\n",
    "for row in tqdm(outer_df.itertuples(), total=len(outer_df)):\n",
    "    mining_date_dir = mining_dir / row.date\n",
    "    read_glob = '%04d-*-read.html' % (row.ranking)\n",
    "    read_path = next(mining_date_dir.glob(read_glob))\n",
    "\n",
    "    soup = BeautifulSoup(open(read_path).read(), 'html5lib')\n",
    "    if soup.select_one('#content > .error_page'):\n",
    "        new_row = [None for _ in columns]\n",
    "    else:\n",
    "        new_row = parse_row_from_sports(soup, office=row.office, remove_metadata=True)\n",
    "    new_rows.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date_input</th>\n",
       "      <th>date_modify</th>\n",
       "      <th>img</th>\n",
       "      <th>vod</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[롤챔스 핫매치 리뷰] '불량 학생' 노동현의 수능 대박 스토리</td>\n",
       "      <td>20150801 12:49</td>\n",
       "      <td>20150801 12:49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"탈선했어요!\". \"수능 만점!\". 2015년 7월 31일, KOO 타이거즈와 kt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[롤챔스 섬머] 스베누 소닉붐 뉴클리어 - 사신 인터뷰, 앞으로는 승리만 안겨드리겠다</td>\n",
       "      <td>20150801 23:32</td>\n",
       "      <td>20150801 23:32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>금일(1일) 롤챔스 섬머 시즌 사상 스베누 소닉붐이 첫 번째 승리를 기록했다. 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[롤챔스 섬머] 뱅 배준식 인터뷰. 솔로랭크를 올리는데는 멘탈이 중요!</td>\n",
       "      <td>20150801 20:10</td>\n",
       "      <td>20150801 20:10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Q. 오늘 승리한 소감은 어떻게 되나?. 오늘 경기에 오랜만에 배성웅(벵기)형와 이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[롬챔스 섬머] 스베누 소닉붐, 꼴찌의 반격 개시! 그 동안 설움을 설욕한 대망의 ...</td>\n",
       "      <td>20150801 23:31</td>\n",
       "      <td>20150801 23:31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>롤챔스 섬머 2경기 2세트는 기세를 올린 스베누 소닉붐이 승리를 가져갔다. 이 때문...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[롬챔스 섬머] 스베누 소닉붐, 한타 대승 기반으로 2세트 승리</td>\n",
       "      <td>20150801 21:58</td>\n",
       "      <td>20150801 21:58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>롤챔스 섬머 2경기 2세트가 시작됐다. 1세트때 진에어 그린윙스에게 억눌린 스베누 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51811</th>\n",
       "      <td>[LCK 승강전] 다이브 받아치면 곧 승리! 샌드박스, 서라벌에 1세트 선승</td>\n",
       "      <td>20200430 17:49</td>\n",
       "      <td>20200430 17:56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30일 종각 LoL 파크에서 열린 열린 2020 LCK 섬머 스플릿 승강전 최종전 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51812</th>\n",
       "      <td>[롤챔스 승강전] 서라벌-샌드박스, 베스트 라인업으로 맞대결</td>\n",
       "      <td>20200430 16:14</td>\n",
       "      <td>20200430 16:31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>마지막 승강전에서 승리를 원하는 서라벌 게이밍과 샌드박스 게이밍이 승강전 내내 기용...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51813</th>\n",
       "      <td>‘일방적인 전투 승리’…샌드박스게이밍, 서라벌게이밍 상대로 1세트 승리</td>\n",
       "      <td>20200430 17:50</td>\n",
       "      <td>20200430 17:50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30일 ‘2020 우리은행 LCK Spring Split’ 승강전 최종진출전에서는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51814</th>\n",
       "      <td>[LCK 승강전] 샌드박스, 절묘한 카운터로 손쉽게 기선 제압</td>\n",
       "      <td>20200430 18:05</td>\n",
       "      <td>20200430 18:05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LCK 최후의 승강전에서 샌드박스가 먼저 웃었다. 다이브 공세를 절묘하게 받아치면서...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51815</th>\n",
       "      <td>[LCK 승강전] 샌드박스, 집중력 앞세워 혼전 승리 2-0 리드</td>\n",
       "      <td>20200430 18:51</td>\n",
       "      <td>20200430 18:51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>잡은 기회는 확실하게 놓치지 않았다. 샌드박스가 LCK 최후의 승강전 생환까지 단 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51816 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title      date_input  \\\n",
       "0                    [롤챔스 핫매치 리뷰] '불량 학생' 노동현의 수능 대박 스토리  20150801 12:49   \n",
       "1        [롤챔스 섬머] 스베누 소닉붐 뉴클리어 - 사신 인터뷰, 앞으로는 승리만 안겨드리겠다  20150801 23:32   \n",
       "2                [롤챔스 섬머] 뱅 배준식 인터뷰. 솔로랭크를 올리는데는 멘탈이 중요!  20150801 20:10   \n",
       "3      [롬챔스 섬머] 스베누 소닉붐, 꼴찌의 반격 개시! 그 동안 설움을 설욕한 대망의 ...  20150801 23:31   \n",
       "4                    [롬챔스 섬머] 스베누 소닉붐, 한타 대승 기반으로 2세트 승리  20150801 21:58   \n",
       "...                                                  ...             ...   \n",
       "51811         [LCK 승강전] 다이브 받아치면 곧 승리! 샌드박스, 서라벌에 1세트 선승  20200430 17:49   \n",
       "51812                  [롤챔스 승강전] 서라벌-샌드박스, 베스트 라인업으로 맞대결  20200430 16:14   \n",
       "51813            ‘일방적인 전투 승리’…샌드박스게이밍, 서라벌게이밍 상대로 1세트 승리  20200430 17:50   \n",
       "51814                 [LCK 승강전] 샌드박스, 절묘한 카운터로 손쉽게 기선 제압  20200430 18:05   \n",
       "51815               [LCK 승강전] 샌드박스, 집중력 앞세워 혼전 승리 2-0 리드  20200430 18:51   \n",
       "\n",
       "          date_modify  img  vod  \\\n",
       "0      20150801 12:49  1.0  0.0   \n",
       "1      20150801 23:32  1.0  0.0   \n",
       "2      20150801 20:10  1.0  0.0   \n",
       "3      20150801 23:31  1.0  0.0   \n",
       "4      20150801 21:58  1.0  0.0   \n",
       "...               ...  ...  ...   \n",
       "51811  20200430 17:56  1.0  0.0   \n",
       "51812  20200430 16:31  1.0  0.0   \n",
       "51813  20200430 17:50  1.0  0.0   \n",
       "51814  20200430 18:05  1.0  0.0   \n",
       "51815  20200430 18:51  1.0  0.0   \n",
       "\n",
       "                                                contents  \n",
       "0      \"탈선했어요!\". \"수능 만점!\". 2015년 7월 31일, KOO 타이거즈와 kt...  \n",
       "1      금일(1일) 롤챔스 섬머 시즌 사상 스베누 소닉붐이 첫 번째 승리를 기록했다. 15...  \n",
       "2      Q. 오늘 승리한 소감은 어떻게 되나?. 오늘 경기에 오랜만에 배성웅(벵기)형와 이...  \n",
       "3      롤챔스 섬머 2경기 2세트는 기세를 올린 스베누 소닉붐이 승리를 가져갔다. 이 때문...  \n",
       "4      롤챔스 섬머 2경기 2세트가 시작됐다. 1세트때 진에어 그린윙스에게 억눌린 스베누 ...  \n",
       "...                                                  ...  \n",
       "51811  30일 종각 LoL 파크에서 열린 열린 2020 LCK 섬머 스플릿 승강전 최종전 ...  \n",
       "51812  마지막 승강전에서 승리를 원하는 서라벌 게이밍과 샌드박스 게이밍이 승강전 내내 기용...  \n",
       "51813  30일 ‘2020 우리은행 LCK Spring Split’ 승강전 최종진출전에서는 ...  \n",
       "51814  LCK 최후의 승강전에서 샌드박스가 먼저 웃었다. 다이브 공세를 절묘하게 받아치면서...  \n",
       "51815  잡은 기회는 확실하게 놓치지 않았다. 샌드박스가 LCK 최후의 승강전 생환까지 단 ...  \n",
       "\n",
       "[51816 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner_df = pd.DataFrame(new_rows, columns=columns)\n",
    "inner_df.to_csv(preprocessing_dir / 'news-inner.csv', index=False)\n",
    "inner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORTER_PATTERN = re.compile(r'[가-힣]{2,4} ?기자')\n",
    "\n",
    "def remove_office_info(x):\n",
    "    office_pattern = re.compile(r'\\w*?%s\\w*' % office)\n",
    "    x = x.strip()\n",
    "    x = office_pattern.sub(' ', x)\n",
    "    x = REPORTER_PATTERN.sub(' ', x)\n",
    "    x = SPACES_PATTERN.sub(' ', x)\n",
    "    return x.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_df[inner_df.contents.apply(lambda x: '[UNCHANGED!!!]' in x if x is not None else False)].contents.to_csv(preprocessing_dir / 'temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_df = pd.read_csv(preprocessing_dir / 'news-inner.csv')\n",
    "for i, c in pd.read_csv(preprocessing_dir / 'temp.csv', index_col=0).itertuples():\n",
    "    inner_df.at[i, 'contents'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_df.contents = inner_df.contents.apply(lambda x: re.sub('  +', ' ', x) if x and x is not np.nan else np.nan)\n",
    "inner_df.contents = inner_df.contents.apply(lambda x: x.replace('?.', '?').replace('!.', '!') if x is not np.nan else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_df.to_csv(preprocessing_dir / 'news-inner.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
